{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating the covariance for dcGAN generated data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we show how to estimate the population covariances $(\\Psi, \\Phi, \\Omega)$ with a Monte Carlo algorithm for a given data generator $\\mathcal{G}$, and teacher-student feature maps $(\\varphi_{t}, \\varphi_{s})$. Concretely, in this notebook we will look at the following setting:\n",
    "\n",
    "- **Generator:** Our generator will be the dcGAN from [Radford et al.](https://arxiv.org/abs/1511.06434) trained to map i.i.d. Gaussian noise $z\\sim\\mathcal{N}(0,\\rm{I}_{100})\\mapsto x\\in\\mathbb{R}^{D}$ into CIFAR10-looking images. For more details, check notebook `synthetic_data_pipeline.ipynb`.\n",
    "- **Teacher features:** The teacher feature map $\\varphi_{t}:x\\in\\mathbb{R}^{D}\\mapsto u\\in\\mathbb{R}^{p}$ will be a fully-connected neural network trained to classify odd (+1) vs even (-1) real CIFAR10 images. The feature map is obtained by selecting all but the last layer, which define the teacher weights $\\theta_{0}\\in\\mathbb{R}^{p}$.\n",
    "- **Student features:** The student feature map $\\varphi_{s}:x\\in\\mathbb{R}^{D}\\mapsto u\\in\\mathbb{R}^{d}$ will be a fully-connected neural network trained on 30k fake CIFAR10-like images sampled from the generator above, with lablels assigned by the teacher also described above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from dcgan import Generator\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more info on the generator, see `synthetic_data_pipeline.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (main): Sequential(\n",
      "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): ConvTranspose2d(64, 3, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (13): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Load the generator\n",
    "\n",
    "latent_dim = 100 # generator latent dimension\n",
    "\n",
    "generator = Generator(ngpu=1)\n",
    "generator.load_state_dict(torch.load(\"./data/weights/dcgan_cifar10_weights.pth\", map_location=device))\n",
    "\n",
    "print(generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the teacher feature map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import teachers\n",
    "import teacherutils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more info on the teacher, see `synthetic_data_pipeline.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (preprocess1): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "  (preprocess2): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "  (preprocess3): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "  (bnz): BatchNorm1d(3072, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "  (fc): Linear(in_features=3072, out_features=1, bias=False)\n",
      "  (v): Linear(in_features=1, out_features=1, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "dx, dy, c = 32, 32, 3 # teacher input dimension\n",
    "D = dx*dy*c\n",
    "p = D\n",
    "# Load teacher vector\n",
    "kwargs = {\"input_dim\": [1, dx, dy]}\n",
    "teacher_mlp = teacherutils.get_model(\"mlp\", \"erf\", D, 1, **kwargs)\n",
    "teacher_mlp.load_state_dict(torch.load(\"./data/weights/mlp_erf_cifar10.pt\", \n",
    "                                       map_location=device))\n",
    "\n",
    "print(teacher_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the feature map. Note inputs are assumed to be flattened\n",
    "teacher_map = lambda x: teacher_mlp.preprocess(x).detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading student feature maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load student\n",
    "d = D\n",
    "student = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D, p, bias=False),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(p,p, bias=False),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(p,1, bias=False))\n",
    "\n",
    "# Load weights.\n",
    "student.load_state_dict(torch.load('./data/weights/weights_mlp_student_epochs=200.pth', map_location=device))\n",
    "\n",
    "# Extract feature map\n",
    "student_map = lambda x: student[:-1](x).detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte-Carlo estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'mean_u': np.zeros(p), \n",
    "        'mean_v': np.zeros(d), \n",
    "        'psi': np.zeros((p,p)), \n",
    "        'omega': np.zeros((d,d)), \n",
    "        'phi': np.zeros((p,d))\n",
    "        }\n",
    "\n",
    "M2_omega = np.zeros((d, d))  # running estimate of residuals\n",
    "M2_phi = np.zeros((p, d))  # running estimate of residuals\n",
    "M2_psi = np.zeros((p, p))  # running estimate of residuals\n",
    "\n",
    "# Keeping last values\n",
    "data_last = {}\n",
    "for name in data.keys():\n",
    "    data_last[name] = np.zeros(data[name].shape)\n",
    "        \n",
    "mc_mean_v_old = 0\n",
    "mc_mean_u_old = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = -1\n",
    "mc_steps = int(1e6) # Maximum number of steps\n",
    "batch_size = 1000 # Number of samples at every batch\n",
    "checkpoint = 100 # Save partial results every checkpoint loops\n",
    "with torch.no_grad():\n",
    "    while step < mc_steps:\n",
    "        for _ in tqdm(range(checkpoint)):\n",
    "            \n",
    "            step += 1\n",
    "            \n",
    "            # Generate CIFAR10-like images\n",
    "            Z = torch.randn(batch_size, latent_dim, 1, 1).to(device)\n",
    "            X = generator(Z).reshape(batch_size, -1)\n",
    "            \n",
    "            # Compute student features\n",
    "            V = student_map(X)\n",
    "            \n",
    "            # Compute teacher faeatures\n",
    "            U = teacher_map(X)\n",
    "            \n",
    "            # Save old means\n",
    "            mc_mean_v_old = data[\"mean_v\"]\n",
    "            mc_mean_u_old = data[\"mean_u\"]\n",
    "\n",
    "            # Update means\n",
    "            dmean_u = np.mean(U, axis=0) - data[\"mean_u\"]\n",
    "            data[\"mean_u\"] += dmean_u / (step + 1)\n",
    "\n",
    "            dmean_v = np.mean(V, axis=0) - data[\"mean_v\"]\n",
    "            data[\"mean_v\"] += dmean_v / (step + 1)\n",
    "\n",
    "            # Update residuals\n",
    "            M2_omega += (V - mc_mean_v_old).T @ (V - data[\"mean_v\"]) / batch_size\n",
    "            M2_psi += (U - mc_mean_u_old).T @ (U - data[\"mean_u\"]) / batch_size\n",
    "            M2_phi += (U - mc_mean_u_old).T @ (V - data[\"mean_v\"]) / batch_size\n",
    "\n",
    "        data[\"omega\"] = M2_omega / (step + 1)\n",
    "        data[\"phi\"] = M2_phi / (step + 1)\n",
    "        data[\"psi\"] = M2_psi / (step + 1)\n",
    "\n",
    "        # Build status message\n",
    "        status = \"{}\".format(step * batch_size)\n",
    "            \n",
    "        for name in data.keys():\n",
    "            diff = np.sqrt(np.mean((data[name] - data_last[name]) ** 2))\n",
    "            status += \", {}\".format(diff)\n",
    "\n",
    "            # Update last\n",
    "            data_last[name] = data[name]\n",
    "        \n",
    "        print(status)\n",
    "        \n",
    "        for name in data.keys():\n",
    "            fname = \"./data/matrices/covariances/{}_t=mlp_s=mlp_epoch=200_n={}.npy\".format(name, step * batch_size)\n",
    "            np.save(fname, data[name])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
